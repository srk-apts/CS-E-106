---
title: "Lab 04"
author: "CSCI E-106 TA's"
date: "10/03/2019"
output:
  pdf_document: default
  html_document: default
---



```{r,echo=FALSE,message=FALSE,warning=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE, 
                      fig.width=10, fig.height=5)

# Function to load or download libraries
loadLib = function(libName)
{
    if(require(libName, character.only=TRUE))
    {
        cat(libName, "loaded properly\n")
    } else {
        cat("Installing", libName, "\n")
        install.packages(libName)
        if(require(libName, character.only=TRUE))
        {
            cat(libName, "loaded properly\n")
        } else {
            stop(c(libName, "not properly installed\n"))
        }
    }
}

libs = c("ggplot2", "knitr", "MASS", "formatR")

for (lib in libs)
{
    loadLib(lib)
}
```

## (Textbook 3.3) Refer to Grade point average Problem 1.19. 

**Please use dataset titled: **CH01PR19.txt** 

a. Prepare a box plot for the ACT scores Xi. Are there any noteworthy features in this plot?

```{r problem 3.3a}
Dataset_1_19 = read.table("CH01PR19.txt", 
                          header=FALSE, sep="", 
                          col.names=c("V1","V2"))

boxplot(Dataset_1_19$V2,main="ACT Scores")

```

Part A Conclusion: We do not see any outliers. We see symmetric distributions in this case. 

b. Prepare a dot plot of the residuals. What information does this plot provide?
```{r problem 3.3b}

ggplot(Dataset_1_19, aes(x =V2, fill = "red")) + geom_dotplot(dotsize = 0.7)

```

Part B Conclusion: Again, we do not see any outliers. We see symmetric distributions in this case.  

c. Plot the residual $e_i$ against the fitted values $Yhat_i$. What departures from regression model (2.1)
can be studied from this plot? What are your findings?

```{r problem 3.3c}

lmfit19 =lm(V1 ~ V2, data = Dataset_1_19)

plot(lmfit19$fitted.values, lmfit19$residuals, xlab="Fitted GPA", ylab="Residuals")
```

Part C Conclusion: We do not see any outliers or any non-linearity in our plot. Thus, we can say that we have a constant variance. 

d. Prepare a normal probability plot of the residuals. Also obtain the coefficient of correlation between the ordered residuals and their expected values under normality. Test the reasonableness of the normality assumption here using Table B.6 and a = .05. What do you conclude?
```{r problem 3.3d}

summary(lmfit19)

ei = lmfit19$residuals

ri = rank(ei)

zr = (ri-0.375)/(120+0.25)
print(zr)

#residual standard error = .6231 which is found from our summary
zr1 = sqrt(0.6231)*qnorm(zr)

cor.test(zr,zr1)

plot(ri,zr1,xlab="ordered Residuals",ylab="Expected Value under Normality")
```
Part D Conclusion: 
So we see our rse: .6231  
H0: Normal Ha: Not normal  
r = .987 
If rse >= .987 conclude H0, otherwise Ha. 
So we conclude Ha. 

e. Conduct the Brown-Forsythe test to determine whether or not the error variance varies with
the level of X. Divide the data into the two groups, X < 26, X >= 26, and use a = .01. State
the decision rule and conclusion. Does your conclusion support your preliminary findings
in part (c)? 
```{r problem 3.3e}

summary(lmfit19)

df = data.frame(cbind(Dataset_1_19[,1],Dataset_1_19[,2],ei))
df1 = df[df[,2]<26,]
df2 = df[df[,2]>=26,]

summary(df1[,3])

summary(df2[,3])

#n1
n1 = length(df1[,3])
print(n1)

#n2
n2 = length(df2[,3])
print(n2)

d1 = abs(df1[,3]+0.032900)
d2 = abs(df2[,3]-0.142618)

#calculate means for our answer 
mean(d1)
mean(d2)

s2 = (var(d1)*(65-1)+var(d2)*(55-1))/(120-2)
print(s2)

#calculate s
s = sqrt(s2)
print(s)

#testStastic = (mean.d1 - mean.d2) / (s * sqrt((1/n1)+1/n2)
testStastic = (.43796-.50652)/(.417275*sqrt((1/65)+(1/55)))  
print(testStastic)

t = qt(.995, 118) 
print(t)
```
Part E Notes: We need to put our answer together 

n1 = 65, mean.d1 = .43796
n2 = 55, mean.d2 =.50652 
s = .417275
test Statistic = (.43796 - .50652)/ .417275 q(1/65)+(1/55)  = -.8968005
t =(.995,118) = 2.61814

Part E Conclusion:  If abs(testStatistic) <= 2.61814 conclude error variance constant, otherwise  error variance not constant. Thus, we conclude error variance constant. 

F. Information is given below for each student on two variables not included in the model, namely, intelligence test score (X2) and high school class rank percentile (X3). (Note that larger class rank percentiles indicate higher standing in the class, e.g., 1 % is near the bottom of the class and 99% is near the top of the class.) Plot the residuals against X2 and X3 on separate graphs to ascertain whether the model can be improved by including either of these variables. What do you conclude? 

*Please use dataset titled: **CH03PR03.txt**
```{r problem 3.3f}

Dataset_3_3 = read.table("CH03PR03.txt", 
                          header=FALSE, sep="", 
                          col.names=c("V1","V2", "V3", "V4"))

plot(Dataset_3_3$V3,ei, xlab="Intelligence Test Score", ylab="Residuals")
plot(Dataset_3_3$V4,ei, xlab="High School Class Rank", ylab="Residuals")

```
Part F Conclusion: X2 is highly correlated with error term, but X3 doesnâ€™t show any correlation or pattern. X2 could be added to the model. 

## (Textbook 3.4) Refer to Copier maintenance Problem 1.20

*Please use dataset titled: **CH01PR20.txt***

>

```{r problem 3.4 load data}

# Semi-descriptive var names for DF and cols 
Dataset_1_20 = read.table("CH01PR20.txt", 
                          header=FALSE, sep="", 
                          col.names=c("V1","V2"))

```
a. Prepare a dot plot for the number of copiers serviced XI. What information is
provided by this plot? Are there any outlying cases with respect to this variable?
```{r problem 3.4a}
par(mfrow=c(1,2))
ggplot(Dataset_1_20, aes(x = V1, fill = "red")) + geom_dotplot(dotsize = 0.7)
ggplot(Dataset_1_20, aes(x = V2, color = "black")) + geom_dotplot(dotsize = 0.7)
```

> Note: There are no outliers here on either plots.  


b. The cases are given in time order. Prepare a time plot for the number of copiers serviced. What does your plot show?
```{r problem 3.4b}

plot(Dataset_1_20$V2,type="b",col="blue",xlab="Run", ylab="Number of Copiers Serviced")
```

> We do not see a time effect. 


c. Prepare a stem-and-leaf plot of the residuals. Are there any noteworthy features in this plot?
```{r problem 3.4c}

stem(Dataset_1_20$V2)

stem(Dataset_1_20$V1)

```
> We do not see any outliers with the plot of the residuals. If anything, it is roughly normal or a little slightly right skewed. 


d. Prepare residual plots of ei versus Y; and ei versus Xi on separate graphs. Do these plots provide the same information? What departures from regression model (2.1) can be studied from these plots? State your findings.
```{r problem 3.4d}

f_1_20 = lm(V1~V2,data=Dataset_1_20)
ei = f_1_20$residuals
yhat= f_1_20$fitted.values
yi= Dataset_1_20$V1
xi= Dataset_1_20$V2

par(mfrow=c(1,2))
plot(ei,yhat)
plot(xi,yhat)
plot(ei,yhat)
plot(ei,xi)
plot(ei,yi)

```


> In this case if you compare them then most of the plots look identical. 


e. Prepare a normal probability plot of the residuals. Also obtain the coefficient of correlation between the ordered residuals and their expected values under normality. Does the normality assumption appear to be tenable here? Use Table B.6 and a = .10.
```{r problem 3.4e}
#there are two ways that this can be done 

#long way to do this: 

anova(f_1_20)
MSE = 79

summary(f_1_20)
ei_rank = rank(ei)
z1 = (ei_rank - 0.375)/(45+0.25)
exp_rank = sqrt(MSE)*qnorm(z1)
part_e = data.frame(ei,ei_rank,z1,exp_rank)

#see all results
part_e
print(part_e)

#show in a plot
plot(exp_rank, ei)


#short way to do the same as above and plot 
par(mfrow=c(2,2))
plot(f_1_20)


#getting correlation information 
cor.test(exp_rank,ei)
```
> We see here the distribution is normal with no outliers. 
> We also reject the null as it is normal. 

f. Prepare a time plot of the residuals to ascertain whether the error terms are correlated over time. What is your conclusion?
```{r problem 3.4f}

plot(ei,type="b",col="blue",xlab="Run", ylab="Error")

```

> We see no correlation with time.

g. Assume that (3.10) is applicable and conduct the Breusch-Pagan test to determine whether or not the error variance varies with the level of X. Use a = .05. State the alternatives,decision rule, and conclusion.
```{r problem 3.4g}

ei2 = ei^2
f = lm(ei2~xi)
summary(f)

#to find SSE(R) and SSR(R)
anova(lm(ei2~xi))

#to find SSE(F) and SSR(F)
anova(f_1_20)

#chi-squared: [SSR(R)/2] / [SSE/n]^2  
chi = (15155/2) / ((3416/45))^2
print(chi)

#p 
p = 1-pchisq(1.314968,2,45)
print(p)
```

> SSR(R) = 15155 
SSE(R) = 46556 
df = 43 


> SSR(F) = 76960
SSE(F)= 3416
df= 43


> After all of our above we would see that we will accept our null as the error variance is constant.



## (Textbook 3.11) Drug concentration. 
A pharmacologist employed linear regression model (2.1) to study the relation between the concentration of a drug in plasma (Y) and the log-dose of the drug (X).The residuals and log-dose levels follow. 

*Please use dataset titled: **CH03PR11.txt**

a. Plot the residuals ei against Xi. What conclusions do you draw from the plot?

```{r problem 3.11a}
Dataset_3_11 = read.table("CH03PR11.txt", 
                          header=FALSE, sep="", 
                          col.names=c("V1","V2"))


plot(Dataset_3_11$V1,Dataset_3_11$V2, xlab="Log-Dose", ylab="Residuals")
```
Part A Conclusion: We see a non-constant variance here. 


b. Assume that (3.10) is applicable and conduct the Breusch-Pagan test to determine whether or not the error variance varies with log-dose of the drug (X). Use a = .05. State the alternatives, decision rule, and conclusion. Does your conclusion support your preliminary findings in part (a)? 
```{r problem 3.11b}

lmfit3_11 = lm(Dataset_3_11$V2^2~Dataset_3_11$V1)
summary(lmfit3_11)
anova(lmfit3_11)

sse = sum(Dataset_3_11$V2^2)
print(sse)

#figure out chisquared
chisq = qchisq(.95,1)
print(chisq)

#test stat: 
x2 = (330.04/2) / (59.96/9)^2
print(x2)

```

Part B Notes: 
Ho: Error Variance is constant Ha: Error Variance is not constant
 
SSR = 330.04 (from summary above)
SSE = 59.96 (from sse calculation above)

test statistic: (from calculation above)
X^2 = (330.04/2) / (59.96/9)^2 = 3.717906 

Chi-squared: 3.84 

Part B Conclusion: If X^2 <= 3.84 conclude error variance constant, otherwise error variance not constant. 
So we conclude error variance not constant.

## (Textbook 3.18)Production time. In a manufacturing study, the production times for 111 recent production runs were obtained. The table below lists for each run the production time in hours (Y) and the production lot size (X). 

*Please use dataset titled: **CH03PR18.txt**

a. Prepare a scatter plot of the data. Does a linear relation appear adequate here? Would a transformation on X or Y be more appropriate here? Why?
```{r problem 3.18a}

Dataset_3_18 = read.table("CH03PR18.txt", 
                          header=FALSE, sep="", 
                          col.names=c("V1","V2"))

plot(Dataset_3_18$V2,Dataset_3_18$V1, xlab ="Lot Size", ylab="Time in Hours")

```

Part A Conclusion: A transformation to X is more suitable because the different levels seem to be pretty consistent. 


b. Use the transformation X' = -JX and obtain the estimated linear regression function for the transformed data.

```{r problem 3.18b}

lmfit3_18 = lm(V1~sqrt(V2), data = Dataset_3_18 )
summary(lmfit3_18)

```

Part B Conclusion: The linear regression function is: y' = 1.2547-3.6235x'


c. Plot the estimated regression line and the transformed data. Does the regression line appear to be a good fit to the transformed data?
```{r problem 3.18c}

plot(sqrt(Dataset_3_18$V2),Dataset_3_18$V1, xlab="Sqrt Lot Size", ylab="Time in Hours")
abline(lmfit3_18)


```

Part C Conclusion: Yes the linear regression appears to be a good fit. 


d. Obtain the residuals and plot them against the fitted values. Also prepare a normal probability plot. What do your plots show?

```{r problem 3.18d}

plot(lmfit3_18$residuals,Dataset_3_18$V1, xlab="Residuals", ylab="Time in Hours")

plot(lmfit3_18)
```

Part D Conclusion: The residuals plot (the first plot) shows that points are spread out without a systematic pattern. The normal probability plot shows that the points all fall pretty close to a straight line. 

e. Express the estimated regression function in the original units.

Part E conclusion: The estimated regression line expressed in original units is: yhat = 1.2547-3.6235*sqrt(x)

## (Textbook 3.25) 
Refer to the CDI data set in Appendix C.2 and Project 1.43. For each of the three fitted regression models, obtain the residuals and prepare a residual plot against X and a normal probability plot. Summarize your conclusions. Is linear regression model (2.1) more appropriate in one case than in the others? 

*Please use dataset titled: **APPENC02.txt***

```{r problem 3.25}

df_cdi = read.table("APPENC02.txt", header=FALSE, sep="", col.names=c("id", "county", "state", "landArea", "totPop", 
                     "percAge18_34", "percAge65plus", "actPhysicians",
                     "hospBeds", "totSerCrimes", "percHSgrads", "percBachDeg",
                     "percBelowPov", "percUnempl", "perCapitaInc", 
                     "totPersIncome", "geoRegion"))


f_3.25_1 = lm(df_cdi$actPhysicians~df_cdi$totPop)
f_3.25_2 = lm(df_cdi$actPhysicians~df_cdi$hospBeds)
f_3.25_3 = lm(df_cdi$actPhysicians~df_cdi$totPersIncome)

e1 = f_3.25_1$residuals
e2 = f_3.25_2$residuals
e3 = f_3.25_3$residuals

#standardize residuals needed for QQ plot##
rs1 = rstandard(f_3.25_1)
rs2 = rstandard(f_3.25_2)
rs3 = rstandard(f_3.25_3)

par(mfrow=c(2,3))

plot(df_cdi$totPop,e1, xlab="Total Population", col="red")
plot(df_cdi$hospBeds,e2, xlab="Hospital Beds", col="blue")
plot(df_cdi$totPersIncome,e3, xlab="Total Personal Incomce", col="green")

## QQ plot ##
qqnorm(rs1)
qqnorm(rs2)
qqnorm(rs3)
```

Problem 3.25 Conclusion: There does not seem to be one that is more visually significant than the others.


## (Textbook 3.32) 
Refer to the Prostate cancer data set in Appendix C.5. Build a regression model to predict PSA level (Y) as a function of cancer, Volume (X). The analysis should include an assessment of the degree to which the key regression assumptions are satisfied. If the regression assumptions are not met, include and justify appropriate remedial measures. Use the final model to estimate mean PSA level for a patient whose cancer volume is 20 cc. Assess the strengths and weaknesses of the final model. 

*Please use dataset titled: **APPENC05.txt***
```{r problem 3.32}

df_prostate = read.table("APPENC05.txt", header=FALSE, sep="", col.names=c("id", "psa", "cancerVol", "weight", "age", "benPros", "seminalVes", "capPen","gleasonScore"))


f_3.32 = lm(df_prostate$psa ~ df_prostate$cancerVol)
summary(f_3.32)


e1 = f_3.32$residuals

#standardize residuals needed for QQ plot##
rs1 = rstandard(f_3.32)

par(mfrow=c(1,3))

plot(df_prostate$cancerVol, df_prostate$psa, xlab="Cancer Volume", ylab="PSA")
abline(f_3.32)

plot(df_prostate$cancerVol,e1, xlab="Cancer Valume", ylab="Residuals")
abline(f_3.32)

## QQ plot ##
qqnorm(rs1)

par(mfrow=c(1,1))

boxcox(f_3.32,lambda=seq(-5,5,by=.1))

#From our boxcox we see that we need to use the log transformation

f_3.32_2 = lm(log(df_prostate$psa)~ df_prostate$cancerVol)
summary(f_3.32_2)

e1 = f_3.32_2$residuals

#standardize residuals needed for QQ plot##
rs1 = rstandard(f_3.32_2)

par(mfrow=c(1,3))

plot(df_prostate$cancerVol, log(df_prostate$psa), xlab="Cancer Volume", ylab="PSA")

abline(f_3.32_2)

plot(df_prostate$cancerVol,e1, xlab="Cancer Volume")

abline(f_3.32_2)

qqnorm(rs1)

```

Problem 3.32: It still seems like we were getting a lot of outliers in our 2 graphs. Our normal plot is in line but not still not completely a great line. 

