---
title: '**CS-E-106: Data Modeling**'
subtitle: '**Assignment 5**'
date: '**Due Date:** 11/04/2019'
author:
  - '**Instructor: Hakan Gogtas**'
  - '**Submitted by:** Saurabh Kulkarni'
output: pdf_document
---

```{r}

library(ggplot2)
library(MASS)
library(lattice)
```


**Solution 1:**

**(a)**

**(1)**

$$ 
X'X = 
\begin{pmatrix} 
n & \sum{X_{i}}\\
\sum{X_{i}} & \sum{X_{i}^{2}}
\end{pmatrix}
$$
Using rule 5.22:
$a = n$
$b = c = \sum{X_{i}}$
$d = \sum{X_{i}^{2}}$

$D = n\sum{X_{i}^{2}} - (\sum{X_{i}})(\sum{X_{i}} = n[\sum{X_{i}^{2}} - \frac{(\sum{X_{i}})^{2}}{n}]) = n\sum{(X_{i}-\bar{X)}^{2}}$


$$ 
(X'X)^{-1} = 
\begin{pmatrix} 
\frac{\sum{X_{i}^{2}}}{n\sum{(X_{i}-\bar{X})^2}} & \frac{-\sum{X_{i}}}{n\sum{(X_{i}-\bar{X})^2}}\\
\frac{-\sum{X_{i}}}{n\sum{(X_{i}-\bar{X})^2}} & \frac{n}{n\sum{(X_{i}-\bar{X})^2}}
\end{pmatrix}
$$

However, $\sum{X_{i}} = n\bar{X}$ and $\sum{(X_{i}-\bar{X})}^2=\sum{X_{i}^2}-n\bar{X}^2$


$$
(X'X)^{-1} = 
\begin{pmatrix} 
\frac{1}{n}+\frac{\bar{X}^2}{\sum{(X_{i}-\bar{X})^2}} & \frac{-\bar{X}}{\sum{(X_{i}-\bar{X})^2}}\\
\frac{-\bar{X}}{\sum{(X_{i}-\bar{X})^2}} & \frac{1}{\sum{(X_{i}-\bar{X})^2}}
\end{pmatrix}
$$

**(2)**

$nb_{0}+b_{1}\sum{X_{i}} = \sum{Y_{i}}$

$b_{0}\sum{X_{i}}+b_{1}\sum{X_{i}^2} = \sum{X_{i}Y_{i}}$

$\implies X'Xb = X'Y$

$\implies b = (X'X)^{-1}X'Y$

**(3)**

Normal error regression model:
$Y_{i} = \beta{0}+\beta_{1}X_{i}+\epsilon_{i}$

$$
Y = \begin{pmatrix} 
Y_{1}\\
Y_{2}\\
.\\
.\\
.\\
Y_{n}
\end{pmatrix}

X = \begin{pmatrix} 
1 & X_{1}\\
1 & X_{1}\\
. & .\\
. & .\\
. & .\\
1 & X_{n}
\end{pmatrix}

\beta = 
\begin{pmatrix} 
\beta_{0}\\
\beta_{1}
\end{pmatrix}

\epsilon = \begin{pmatrix} 
\epsilon_{1}\\
\epsilon_{2}\\
.\\
.\\
.\\
\epsilon_{n}
\end{pmatrix}
$$

$$
\begin{pmatrix} 
Y_{1}\\
Y_{2}\\
.\\
.\\
.\\
Y_{n}
\end{pmatrix} 
= 
\begin{pmatrix} 
1 & X_{1}\\
1 & X_{1}\\
. & .\\
. & .\\
. & .\\
1 & X_{n}
\end{pmatrix} 

\begin{pmatrix} 
\beta_{0}\\
\beta_{1}
\end{pmatrix} 
+ 
\begin{pmatrix} 
\epsilon_{1}\\
\epsilon_{2}\\
.\\
.\\
.\\
\epsilon_{n}
\end{pmatrix}
$$
$$
\therefore \hat{Y} = X \beta
$$


**(4)**


$$
\hat{Y} = \begin{pmatrix} 
Y_{1}\\
Y_{2}\\
.\\
.\\
.\\
Y_{n}
\end{pmatrix}
= Xb = X(X'X)^{-1}X'Y = HY

\implies H = X(X'X)^{-1}X'

$$

**(5)**

$$
e = \begin{pmatrix} 
\epsilon_{1}\\
\epsilon_{2}\\
.\\
.\\
.\\
\epsilon_{n}
\end{pmatrix} 
= 
Y - \hat{Y} = Y-X'b
$$

$$
SSE = e'e = (Y-Xb)'(Y-Xb) = Y'Y-b'X'Y
$$

**(6)**

$$
\sigma^{2}{b} = \sigma^{2}(X'X)^{-1}
$$
$$
\sigma^{2}{b} = \begin{pmatrix} 
\frac{\sigma^{2}}{n}+\frac{\sigma^{2}\bar{X}^2}{\sum{(X_{i}-\bar{X})^2}} & \frac{-\bar{X}\sigma^{2}}{\sum{(X_{i}-\bar{X})^2}}\\
\frac{-\bar{X}\sigma^{2}}{\sum{(X_{i}-\bar{X})^2}} & \frac{\sigma^{2}}{\sum{(X_{i}-\bar{X})^2}}
\end{pmatrix}
$$

$$
s^{2}{b} = MSE(X'X)^{-1} = \begin{pmatrix} 
\frac{MSE}{n}+\frac{MSE\bar{X}^2}{\sum{(X_{i}-\bar{X})^2}} & \frac{-\bar{X}MSE}{\sum{(X_{i}-\bar{X})^2}}\\
\frac{-\bar{X}MSE}{\sum{(X_{i}-\bar{X})^2}} & \frac{MSE}{\sum{(X_{i}-\bar{X})^2}}
\end{pmatrix}
$$

**(7)**

$$
s^2{pred} = MSE(1+X'_{h}(X'X)^{-1}X_{h})
$$

At $X_{h}=30$,

$s^2{pred} = MSE(1+30^2(X'X)^{-1})$

**(b)**

$$
\sigma^{2}{b} = \begin{pmatrix} 
\sigma^{2}(b_{0}) & \sigma(b_{0},b_{1})\\
\sigma(b_{0},b_{1}) & \sigma^{2}(b_{1})
\end{pmatrix}
$$
Thus, from part(a)(6):

$s^2(b_{0}) = \frac{MSE}{n}+\frac{MSE\bar{X}^2}{\sum{(X_{i}-\bar{X})^2}}$

$s(b_{0}, b_{1}) = \frac{-\bar{X}MSE}{\sum{(X_{i}-\bar{X})^2}}$

$s^2(b_{1}) = \frac{MSE}{\sum{(X_{i}-\bar{X})^2}}$

**(c)**

From part(a)(5), 
$$
SSE = e'e = (Y-Xb)'(Y-Xb) = Y'Y-b'X'Y
$$
$b'X' = (Xb)' = \hat{Y}' = (HY)'$

From Hat matrix part(a)(3-4):

$b'X' = (HY)'$

H is symmetric, so H' = H. Hence,

$b'X' = Y'H$

$\implies SSE = Y'(I-H)Y$


**Solution 2:**

**(a)**

```{r}
brand_data = read.csv("Brand Preference.csv")
splom(brand_data)
```


```{r}
library(corrgram)
corrgram(brand_data, order=TRUE, lower.panel=panel.shade,
  upper.panel=panel.pie, text.panel=panel.txt, oma=c(3,3,3,15),
  main="Correlogram")
cor(brand_data)
```

*Interpretation:*

We can see a linear relationship between X1 and Y ($r \approx 0.9$). However, there seems to be little correlation between X2 and Y, or X2 and X1 either.


**(b)**

```{r}
lm_brand = lm(Y~., data=brand_data)
summary(lm_brand)
```

Estimated Regression Function: $Y = 37.65+4.425*X1+4.375*X1$

*Interpretation:*

Based on the regression function, none of the $\beta$'s seem to be zero. $\beta_{2}$ does has a higher standard error and a greater p-value, which means X1 is more correlated to Y compared to X2. Also, the model is a very good fit ($R^2=0.95$).

**(c)**

```{r}
ei = lm_brand$residuals
boxplot(ei)
title(main="Boxplot of Residuals")
```

*Interpretation:*

We see that we don't have any outliers in the error term based on the box plot. Also, it seems to be evenly spread around 0. 

**(d)**

```{r}
plot(lm_brand$fitted.values, ei)
plot(brand_data$X1, ei)
plot(brand_data$X2, ei)
plot(brand_data$X1*brand_data$X2, ei)

```


```{r}
df=brand_data
rse=2.693

ri = rank(ei)
n = nrow(df)
zr = (ri-0.375)/(n+0.25)

#residual standard error from summary(lm) above
zr1 = rse*qnorm(zr)

print(cor.test(zr1, ei))

plot(zr1, ei, xlab="Expected Value under Normality",ylab="Residuals")
title(main="Normal Probability Plot")
```

*Interpretation:*

*Residual Plots:* The residuals appear to be equally spread and have no distinct patterns. We can say that there is contant variance in the error term.

*Normal Probability Plot:* The plot seems to be almost linear, which means that the error is in agreement with the normality. 


**(e)**

Null Hypothesis: $H_{0}$: Error variance is constant
Alternate Hypothesis: $H_{1}$: Error variance is not constant

```{r}
ei2 = ei^2
f = lm(ei2~brand_data$X1+brand_data$X2)
summary(f)

#to find SSE(R) and SSR(R)
anova(f)

#to find SSE(F) and SSR(F)
anova(lm_brand)
```


```{r}

SSR_R = 67.34+5.06
SSE_R = 494.35 

SSR_F = 1566.45+306.25
SSE_F= 94.30

n = nrow(brand_data)

#chi-squared: [SSR(R)/2] / [SSE(F)/n]^2  
chiTest = (SSR_R/2) / ((SSE_F/n))^2
print(chiTest)


#p 
chi = qchisq(1-0.05,1)
print(chi)

```

*Decision Rule:*

- If $chiTest \leq \chi^{2}(1-\alpha,1)$, conclude $H_{0}$: constant error variance

- If $chiTest > \chi^{2}(1-\alpha,1)$, conclude $H_{1}$: non-constant error variance 


*Result:*

Since $1.042138 \leq 3.841459$ i.e. $chiTest \leq \chi^{2}(1-\alpha,1)$, we conclude $H_{0}$. The error variance is constant.


**Solution 3:**

**(a)**

*Hypothesis:*

$H_0: \beta_k = 0$ 

$H_a: \beta_k \neq 0$ 

```{r}
df_brand = lm_brand$df.residual
alpha = 0.01
anova(lm_brand)
```

```{r}
SSR_X1 = anova(lm_brand)[1,2]
SSR_X2 = anova(lm_brand)[2,2]
SSE = anova(lm_brand)[3,2]
MSR_X1 = SSR_X1/1
MSR_X2 = SSR_X2/1
MSE = SSE/df_brand

F_star_X1 = MSR_X1/MSE
print(F_star_X1)

F_star_X2 = MSR_X2/MSE
print(F_star_X2)
```


```{r}
FTest = qf(1-alpha, 1, df_brand) 
print(FTest)

```

*Decision Rule:*

If $F^* \leq FTest$ , conclude $H_0$

If $F^* > FTest$ , conclude $H_a$


*Result:*

Since $F^*_{X1} and F^*_{X2}$ are both $> FTest$, we conclude $H_a$ i.e. both the $\beta_k$ are $neq$ 0. Thus, there exists a linear relation.


**(b)**


```{r}
alpha = 0.01
g = length(lm_brand$coefficients)
confint(lm_brand, level = 1-alpha/g)
```

*Interpretation:*

Family confidence coefficient means that the obtained confidence intervals, for several $\beta_k$, are simultaneously accurate with a confidence coefficient of $1-\alpha = 99$%.

**(c)**

```{r}
Xh<-data.frame(X1=5, X2=4)
predict(lm_brand, Xh,se.fit=TRUE,interval="confidence",level=1-alpha)
```

*Interpretation:*

This means that the $E[Y_h]$ for the observations in $X_h$ are within the obtained interval with a confidence coeficient of $1-\alpha = 99$%, where all observations in $X_h$ are seen by our model.

**(d)**

```{r}
Xh<-data.frame(X1=5, X2=4)
predict(lm_brand, Xh,se.fit=TRUE,interval="prediction",level=0.99)
```

*Interpretation:*

This means that the $E[Y_h]$ for the observations in $X_h$ are within the obtained interval with a confidence coeficient of $1-\alpha = 99$ %, where all observations in $X_h$ are new.


**Solution 4:**

**(a)**

```{r}
properties_data = read.csv("Commercial Properties.csv")
par(mfrow=c(1,1))
splom(properties_data, order=TRUE, oma=c(3,3,3,15))

```


```{r}
corrgram(properties_data, order=TRUE, lower.panel=panel.shade,
  upper.panel=panel.pie, text.panel=panel.txt, oma=c(3,3,3,15),
  main="Correlogram")
cor(properties_data)
```

*Interpretation:*

- There seems to be no 1:1 correlation in the data between any of the variables. 
- The highest 1:1 correlation being between X4 and Y. 
- We can see some clusters of data points in the plots for X1 and X3, showing that they are not equally spread.

**(b)**

```{r}
lm_prop = lm(Y~., data=properties_data)
summary(lm_prop)
```

Regression Function: $Y = 12.2-0.142*X1+0.282*X2+0.6193*X3+7.924e-06*X4$

**(c)**

```{r}
ei = lm_prop$residuals
qqmath(ei)
```

*Interpretation:* The QQ plot seems to be almost linear, which means that the error is in agreement with the normality. Distribution is fairly linear.

**(d)**

```{r}
df = properties_data
rse = 1.137

ri = rank(ei)
n = nrow(df)
zr = (ri-0.375)/(n+0.25)

#residual standard error from summary(lm) above
zr1 = rse*qnorm(zr)

print(cor.test(zr1, ei))

plot(zr1, ei, xlab="Expected Value under Normality",ylab="Residuals")
title(main="Normal Probability Plot")
```

*Interpretation:* The normal probability plot also seems to be almost linear, which means that the error is in agreement with the normality. 


```{r}
plot(properties_data$Y, ei)
plot(properties_data$X1, ei)
plot(properties_data$X2, ei)
plot(properties_data$X3, ei)
plot(properties_data$X4, ei)
plot(properties_data$X1*properties_data$X2, ei)
plot(properties_data$X1*properties_data$X3, ei)
plot(properties_data$X1*properties_data$X4, ei)
plot(properties_data$X2*properties_data$X3, ei)
plot(properties_data$X2*properties_data$X4, ei)
plot(properties_data$X3*properties_data$X4, ei)
```

*Interpretation:* 
- There is a linear pattern between residuals and Y, which could suggest that our linear model is not a good fit through out the data.
- We can see some clusters of data points in the plots for X1 and X3 and their respective interactions.
- For X2 and X4 and their interactions, the error variance is constant.

**(e)**

```{r}
ei = lm_prop$residuals
fitted_df = data.frame(fitted_values = lm_prop$fitted.values)
df = data.frame(cbind(properties_data, fitted_df, ei))
df = df[order(df$fitted_values),]
```


Null Hypothesis: $H_{0}$: Error variance is constant
Alternate Hypothesis: $H_{1}$: Error variance is not constant

```{r}
df1 = df[1:40,]
df2 = df[41:nrow(df),]

med1 = median(df1$ei)
med2 = median(df2$ei)

#n1
n1 = nrow(df1)
print(n1)

#n2
n2 = nrow(df2)
print(n2)

d1 = abs(df1$ei-med1)
d2 = abs(df2$ei-med2)

#calculate means for our answer 
mean_d1 = mean(d1)
print(mean_d1)
mean_d2 = mean(d2)
print(mean_d2)

s2 = (var(d1)*(n1-1)+var(d2)*(n2-1))/(n1+n2-2)
print(s2)

#calculate s
s = sqrt(s2)
print(s)

#testStastic = (mean.d1 - mean.d2) / (s * sqrt((1/n1)+1/n2)
testStastic = (mean_d1-mean_d2)/(s*sqrt((1/n1)+(1/n2)))  
print(testStastic)

t = qt(1-0.05, 118) 
print(t)

```

*Decision Rule:*

- If $|testStatistic| \leq t(1-\alpha/2,n-2)$, conclude $H_{0}$: constant error variance

- If $|testStatistic| > t(1-\alpha/2,n-2)$, conclude $H_{1}$: non-constant error variance 


*Result:*

Since $|0.5520951| \leq 1.65787$ i.e. $|testStatistic| \leq t(1-\alpha/2,n-2)$, we conclude $H_{0}$. The error variance is constant and thus does not vary with X.

**Solution 5:**

**(a)**

```{r}
X1 = c(4.0,6.0,12.0)
X2 = c(10.0,11.5,12.5)
X3 = c(0.1,0,0.32)
X4 = c(80000,120000,340000)
Xh = data.frame(X1, X2, X3, X4)

alpha = 0.05
g = nrow(Xh)

```


```{r}
predict(lm_prop, Xh, se.fit=TRUE, interval="prediction", level=1-alpha)
```

*Interpretation:*

We see that the intervals are not too wide for the individual prediction intervals. We get an $R^2$ of 0.5847 which gives a somewhat of a good prediction.

```{r}
CI = predict(lm_prop, Xh, se.fit=TRUE, interval="confidence", level=1-alpha/g)
CI
```

*Bonferroni and Working-Hotelling Method:*

```{r}
p = length(lm_prop$coefficients)
n = nrow(properties_data)
B = qt(1-alpha/(2*g), n-p)
W = sqrt(2*qf(1-alpha, g, n-p))
s = CI$se.fit
Yh = CI$fit[,1]

est_resp_CI = t(
rbind(
"Xh" = t(Xh),
"fit" = array(Yh),
"lower.B" = array(Yh-B*s),
"upper.B" = array(Yh+B*s),
"lower.W" = array(Yh-W*s),
"upper.W" = array(Yh+W*s)
)
)

est_resp_CI

```


